{"cells":[{"cell_type":"markdown","metadata":{"id":"2o55KrNI7U1H"},"source":["## Assignment 3 ##\n","\n","Your Name:\n","\n","## Assignment Question ##\n","\n","Using the below data set, perform LDA topic modeling to identify multiple latent topics inside.\n","\n","The number of topipcs is not fixed, it is up to you to decide how many topics to go with.\n","\n","## Grading Guidelines: ##\n","\n","You need to show all the steps (Codes & outputs) from uploading the data set to performing topic modeling to derive topics with keywords.\n","\n","DO NOT CLEAR THE OUTPUTS (Leave the outputs printed).\n"]},{"cell_type":"markdown","metadata":{"id":"sD2ZNBg07U1J"},"source":["## Step 1: Load the dataset\n","\n","The dataset we'll use is a list of news headlines published over a period of 15 years. \n","\n","We'll start by loading it from the `abcnews-date-text.csv` file."]},{"cell_type":"code","execution_count":1,"metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"en3ryqgl7U1K","executionInfo":{"status":"ok","timestamp":1678659962183,"user_tz":240,"elapsed":28518,"user":{"displayName":"융융이","userId":"05313301844663708595"}},"outputId":"dff29307-c48b-4c29-9120-ed1b33258bae"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n","   publish_date                                      headline_text\n","0      20030219  aba decides against community broadcasting lic...\n","1      20030219     act fire witnesses must be aware of defamation\n","2      20030219     a g calls for infrastructure protection summit\n","3      20030219           air nz staff in aust strike for pay rise\n","4      20030219      air nz strike to affect australian travellers\n","5      20030219                  ambitious olsson wins triple jump\n","6      20030219         antic delighted with record breaking barca\n","7      20030219  aussie qualifier stosur wastes four memphis match\n","8      20030219       aust addresses un security council over iraq\n","9      20030219         australia is locked into war timetable opp\n","Shape: (1103665, 2)\n","Column Names ['publish_date' 'headline_text']\n"]}],"source":["#connect Colab to your Google Drive.\n","from google.colab import drive\n","import os\n","drive.mount('/content/gdrive')\n","\n","#Load the dataset from the CSV and save it to 'data_text'\n","\n","import pandas as pd\n","news=pd.read_csv('/content/gdrive/My Drive/CIS NLP Data Sets/abcnews-date-text.csv')\n","print (news.iloc[:10,:])\n","#How many columns and rows?\n","print (\"Shape:\", news.shape)\n","#Column names?\n","print (\"Column Names\",news.columns.values)"]},{"cell_type":"markdown","metadata":{"id":"bEcsp0Ru7U1L"},"source":["## Step 2: Data Preprocessing ##\n","\n","We will perform the following steps:\n","\n","The order of the pre-processing steps doesn't have to be in this way.\n","\n","It is up to you whether you start tokenizing first or other processing steps or at the same time.\n","\n","HOWEVER, make sure that all the below steps are performed and applied to the headline text.\n","\n","* **Tokenization** \n","* **Lowercasing** \n","* **remove punctuations**\n","* **Words that have fewer than 3 characters are removed**\n","* **stopwords are removed**\n","* **lemmatized** - words in third person are changed to first person and verbs in past and future tenses are changed into present.\n","\n","**Lemmatization code is give below, use the below code for lemmatization.**"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zu4d6s8z7U1L","executionInfo":{"status":"ok","timestamp":1678660013371,"user_tz":240,"elapsed":1524,"user":{"displayName":"융융이","userId":"05313301844663708595"}},"outputId":"d8bb8e48-154d-487e-bf4f-f8a340145940"},"outputs":[{"output_type":"stream","name":"stdout","text":["do\n"]}],"source":["from nltk.stem import WordNetLemmatizer\n","print(WordNetLemmatizer().lemmatize('did', pos = 'v'))"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H5bzZo0B7U1L","executionInfo":{"status":"ok","timestamp":1678659992930,"user_tz":240,"elapsed":3865,"user":{"displayName":"융융이","userId":"05313301844663708595"}},"outputId":"73142456-57df-4222-f48c-ded5e23d714d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: gensim in /usr/local/lib/python3.9/dist-packages (3.6.0)\n","Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.9/dist-packages (from gensim) (1.22.4)\n","Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.9/dist-packages (from gensim) (1.10.1)\n","Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.9/dist-packages (from gensim) (6.3.0)\n","Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.9/dist-packages (from gensim) (1.15.0)\n"]}],"source":["!pip install gensim\n","import gensim\n","from gensim.utils import simple_preprocess\n","from gensim.parsing.preprocessing import STOPWORDS\n","from nltk.stem import WordNetLemmatizer, SnowballStemmer\n","from nltk.stem.porter import *\n","import numpy as np\n","np.random.seed(400)"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xrF9LqB67U1L","executionInfo":{"status":"ok","timestamp":1678660007539,"user_tz":240,"elapsed":318,"user":{"displayName":"융융이","userId":"05313301844663708595"}},"outputId":"e51823b5-5970-478f-9651-b26a4e3cf08b"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":6}],"source":["import nltk\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7tV_hsKG7U1L","executionInfo":{"status":"ok","timestamp":1678660094566,"user_tz":240,"elapsed":39649,"user":{"displayName":"융융이","userId":"05313301844663708595"}},"outputId":"4e7768cb-220c-4286-a3e6-990f57b3d47c"},"outputs":[{"output_type":"stream","name":"stdout","text":["   publish_date                                      headline_text  \\\n","0      20030219  aba decides against community broadcasting lic...   \n","1      20030219     act fire witnesses must be aware of defamation   \n","2      20030219     a g calls for infrastructure protection summit   \n","3      20030219           air nz staff in aust strike for pay rise   \n","4      20030219      air nz strike to affect australian travellers   \n","5      20030219                  ambitious olsson wins triple jump   \n","6      20030219         antic delighted with record breaking barca   \n","7      20030219  aussie qualifier stosur wastes four memphis match   \n","8      20030219       aust addresses un security council over iraq   \n","9      20030219         australia is locked into war timetable opp   \n","\n","                             headline_text_processed  \n","0            [decide, community, broadcast, licence]  \n","1                       [witness, aware, defamation]  \n","2         [call, infrastructure, protection, summit]  \n","3                        [staff, aust, strike, rise]  \n","4           [strike, affect, australian, travellers]  \n","5             [ambitious, olsson, win, triple, jump]  \n","6             [antic, delight, record, break, barca]  \n","7  [aussie, qualifier, stosur, waste, memphis, ma...  \n","8           [aust, address, security, council, iraq]  \n","9                       [australia, lock, timetable]  \n","Shape: (1103665, 3)\n","Column Names ['publish_date' 'headline_text' 'headline_text_processed']\n"]}],"source":["#lemmatization (simplified version)\n","def lemmatize_text(text):\n","    return WordNetLemmatizer().lemmatize(text, pos='v')\n","\n","# and then remove stop words and words fewer than 2 characters.\n","def preprocess(text):\n","    result = []\n","    for token in gensim.utils.simple_preprocess(text):\n","        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n","            result.append(lemmatize_text(token))\n","    return result\n","\n","news['headline_text_processed'] = news['headline_text'].map(preprocess)\n","\n","print (news.iloc[:10,:])\n","print (\"Shape:\", news.shape)\n","print (\"Column Names\",news.columns.values)\n"]},{"cell_type":"markdown","metadata":{"id":"Wpk4EzWJ7U1M"},"source":["## Step 3: Bag of words on the dataset\n","\n","* 3-1. Dictionary\n","\n","Create a dictionary from pre-processed headline texts containing the number of times a word appears in the training set. \n","\n","To do that, let's pass your pre-processed headline texts to [`gensim.corpora.Dictionary()`](https://radimrehurek.com/gensim/corpora/dictionary.html) and call it '`dictionary`'."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AxhZRWnp7U1N"},"outputs":[],"source":["dictionary = gensim.corpora.Dictionary()"]},{"cell_type":"code","source":["dictionary = gensim.corpora.Dictionary(news['headline_text_processed'])\n","\n","count = 0\n","for k, v in dictionary.iteritems():\n","    print(k, v)\n","    count += 1\n","    if count > 10:\n","        break\n","\n","print (len(dictionary))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4BzOCK8n-BKf","executionInfo":{"status":"ok","timestamp":1678660626213,"user_tz":240,"elapsed":8802,"user":{"displayName":"융융이","userId":"05313301844663708595"}},"outputId":"facf0c60-b443-4712-de77-2a5b5c3ddb73"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["0 broadcast\n","1 community\n","2 decide\n","3 licence\n","4 aware\n","5 defamation\n","6 witness\n","7 call\n","8 infrastructure\n","9 protection\n","10 summit\n","77773\n"]}]},{"cell_type":"markdown","metadata":{"id":"FQrhdjfj7U1N"},"source":["* 3-2. Gensim filter_extremes\n","\n","[`filter_extremes(no_below=i, no_above=j, keep_n=k) where i,j,k can be integers or fractions.`](https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.filter_extremes)\n","\n","Filter out tokens that appear in\n","\n","* less than no_below documents (absolute number) or\n","* more than no_above documents (fraction of total corpus size, not absolute number).\n","* after (1) and (2), keep only the first keep_n most frequent tokens (or keep all if None)."]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dFJnri7E7U1N","executionInfo":{"status":"ok","timestamp":1678660647022,"user_tz":240,"elapsed":302,"user":{"displayName":"융융이","userId":"05313301844663708595"}},"outputId":"c20a5495-42f0-47e6-d715-fde67b152904"},"outputs":[{"output_type":"stream","name":"stdout","text":["1872\n"]}],"source":["#Remove very rare and very common words using filter_extremes():\n","dictionary.filter_extremes(no_below=500, no_above=0.70)\n","print (len(dictionary))"]},{"cell_type":"markdown","metadata":{"id":"IAxzxPhM7U1N"},"source":["* 3-3. Gensim doc2bow\n","\n","* Gensim doc2bow (pass the tokenized words to doc2bow and convert those to vectors.)\n","\n","* Caution: No further preprocessing should be done such as tokenization, lemmatization, and etc before initiating this."]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ceu7hMA47U1N","executionInfo":{"status":"ok","timestamp":1678660709548,"user_tz":240,"elapsed":7262,"user":{"displayName":"융융이","userId":"05313301844663708595"}},"outputId":"7211c5f2-17bb-4ef0-da50-ac02d5309bc7"},"outputs":[{"output_type":"stream","name":"stdout","text":["1103665\n","[(0, 1), (1, 1), (2, 1)]\n"]}],"source":["bow_corpus = [dictionary.doc2bow(doc) for doc in news['headline_text_processed']]\n","print (len(bow_corpus))\n","print (bow_corpus[0])"]},{"cell_type":"markdown","metadata":{"id":"gs0TIP5F7U1O"},"source":["## Step 4: Running LDA using Bag of Words ##\n","\n","Perform LDA model on your final corpus.\n"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"4-wD4oTQ7U1O","executionInfo":{"status":"ok","timestamp":1678661215699,"user_tz":240,"elapsed":390860,"user":{"displayName":"융융이","userId":"05313301844663708595"}}},"outputs":[],"source":["#Run LDA model on the final corpus.\n","\n","#num_topics: the number of latent topics to be extracted from the corpus.\n","#id2word: mapping from word ids (integers) to words (strings).\n","# Some other parameters. See the document explanations for more details.\n","\n","lda_model_tfidf = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary\n","                                             )"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bZsLZhRD7U1O","executionInfo":{"status":"ok","timestamp":1678661215699,"user_tz":240,"elapsed":13,"user":{"displayName":"융융이","userId":"05313301844663708595"}},"outputId":"bfc97800-964d-49f3-baa9-45304f3f15e7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Topic: 0 melbourne attack kill change labor arrest dead report shoot victorian\n","Topic: 1 live tasmanian show interview bank energy make turn rule reveal\n","Topic: 2 australia trump world home open south time family league final\n","Topic: 3 police charge court murder australian death woman jail turnbull accuse\n","Topic: 4 test canberra break lose korea face john talk lead violence\n","Topic: 5 sydney school perth call miss deal national flood work children\n","Topic: 6 say government donald crash die market rise high record price\n","Topic: 7 north adelaide year indigenous win tasmania west concern country vote\n","Topic: 8 election house coast brisbane years power china leave life fight\n","Topic: 9 queensland help hospital fund council health return abuse victoria want\n"]}],"source":["import re\n","for idx, topic in lda_model_tfidf.print_topics(-1, num_words=10):\n","    #print out topic numbers and keywords.\n","#    print('Topic: {} Word: {}'.format(idx, topic))\n","    \n","    #print out keywords only (without probability)\n","    key_words_only = \" \".join(re.findall(\"[a-zA-Z]+\", topic))\n","    print ('Topic:',idx,key_words_only)"]},{"cell_type":"markdown","metadata":{"id":"l1u0ruIL7U1P"},"source":["### Step 5: label the topics ###\n","\n","Using the keywords in each topic , what topics were you able to infer?\n","You should write down the inferred topic labels below.\n","\n","* 0: \n","* 1: \n","* 2: \n","* ..."]},{"cell_type":"code","source":[],"metadata":{"id":"rKpYXdz8_giT"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}