{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lQ85snWxbYtU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2RtosznqbdiU"
   },
   "source": [
    "## Week 7 Topic Modeling (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tAlEWijIbpCE"
   },
   "source": [
    "###connect Colab to your Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23604,
     "status": "ok",
     "timestamp": 1678298036003,
     "user": {
      "displayName": "융융이",
      "userId": "05313301844663708595"
     },
     "user_tz": 300
    },
    "id": "vBOCeX32baoP",
    "outputId": "7ab1b288-42e5-44fe-990f-d887137d9b71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "#connect Colab to your Google Drive.\n",
    "from google.colab import drive\n",
    "import os\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2736,
     "status": "ok",
     "timestamp": 1678301004241,
     "user": {
      "displayName": "융융이",
      "userId": "05313301844663708595"
     },
     "user_tz": 300
    },
    "id": "jjOBthAxPhSk",
    "outputId": "733da995-eeb7-4dc2-ca35-f97531f7a0b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   User_ID                                        Description  \\\n",
      "0  id10326  The room was kind of clean but had a VERY stro...   \n",
      "1  id10327  I stayed at the Crown Plaza April -- - April -...   \n",
      "2  id10328  I booked this hotel through Hotwire at the low...   \n",
      "3  id10329  Stayed here with husband and sons on the way t...   \n",
      "4  id10330  My girlfriends and I stayed here to celebrate ...   \n",
      "5  id10331  We had - rooms. One was very nice and clearly ...   \n",
      "6  id10332  My husband and I have stayed in this hotel a f...   \n",
      "7  id10333  My wife & I stayed in this glorious city a whi...   \n",
      "8  id10334  My boyfriend and I stayed at the Fairmont on a...   \n",
      "9  id10335  Wonderful staff, great location, but it was de...   \n",
      "\n",
      "        Browser_Used Device_Used Is_Response  \n",
      "0               Edge      Mobile   not happy  \n",
      "1  Internet Explorer      Mobile   not happy  \n",
      "2            Mozilla      Tablet   not happy  \n",
      "3   InternetExplorer     Desktop       happy  \n",
      "4               Edge      Tablet   not happy  \n",
      "5   InternetExplorer     Desktop       happy  \n",
      "6            Firefox      Tablet   not happy  \n",
      "7      Google Chrome      Mobile       happy  \n",
      "8  Internet Explorer     Desktop       happy  \n",
      "9             Chrome      Tablet   not happy  \n",
      "Shape: (38932, 5)\n",
      "Column Names ['User_ID' 'Description' 'Browser_Used' 'Device_Used' 'Is_Response']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#import csv file and put it into a Pandas dataframe.\n",
    "hotel=pd.read_csv('/content/gdrive/My Drive/CIS NLP Data Sets/hotel-reviews.csv')\n",
    "\n",
    "\n",
    "#movie.head()\n",
    "print (hotel.iloc[:10,:])\n",
    "\n",
    "#How many columns and rows?\n",
    "print (\"Shape:\", hotel.shape)\n",
    "\n",
    "#Column names?\n",
    "print (\"Column Names\",hotel.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1799,
     "status": "ok",
     "timestamp": 1678301008010,
     "user": {
      "displayName": "융융이",
      "userId": "05313301844663708595"
     },
     "user_tz": 300
    },
    "id": "_x0LJzquPhfF",
    "outputId": "57506b86-4940-4fa0-c0ac-8b80b4f58fcb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IKfA8rubrUSS"
   },
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32068,
     "status": "ok",
     "timestamp": 1678301040077,
     "user": {
      "displayName": "융융이",
      "userId": "05313301844663708595"
     },
     "user_tz": 300
    },
    "id": "l9awPBGxPhhV",
    "outputId": "9c6a8c3a-07f1-4c84-c036-2c44837ba820"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   User_ID                                        Description  \\\n",
      "0  id10326  The room was kind of clean but had a VERY stro...   \n",
      "1  id10327  I stayed at the Crown Plaza April -- - April -...   \n",
      "2  id10328  I booked this hotel through Hotwire at the low...   \n",
      "3  id10329  Stayed here with husband and sons on the way t...   \n",
      "4  id10330  My girlfriends and I stayed here to celebrate ...   \n",
      "5  id10331  We had - rooms. One was very nice and clearly ...   \n",
      "6  id10332  My husband and I have stayed in this hotel a f...   \n",
      "7  id10333  My wife & I stayed in this glorious city a whi...   \n",
      "8  id10334  My boyfriend and I stayed at the Fairmont on a...   \n",
      "9  id10335  Wonderful staff, great location, but it was de...   \n",
      "\n",
      "        Browser_Used Device_Used Is_Response  \\\n",
      "0               Edge      Mobile   not happy   \n",
      "1  Internet Explorer      Mobile   not happy   \n",
      "2            Mozilla      Tablet   not happy   \n",
      "3   InternetExplorer     Desktop       happy   \n",
      "4               Edge      Tablet   not happy   \n",
      "5   InternetExplorer     Desktop       happy   \n",
      "6            Firefox      Tablet   not happy   \n",
      "7      Google Chrome      Mobile       happy   \n",
      "8  Internet Explorer     Desktop       happy   \n",
      "9             Chrome      Tablet   not happy   \n",
      "\n",
      "                    Description_lemmatized_processed  \n",
      "0  [room, kind, clean, strong, smell, dog, genera...  \n",
      "1  [stay, crown, plaza, april, april, staff, frie...  \n",
      "2  [book, hotel, hotwire, lowest, price, desk, ma...  \n",
      "3  [stay, husband, sons, alaska, cruise, love, ho...  \n",
      "4  [girlfriends, stay, celebrate, birthdays, plan...  \n",
      "5  [room, nice, clearly, update, recently, clean,...  \n",
      "6  [husband, stay, hotel, time, fanciest, hotel, ...  \n",
      "7  [wife, stay, glorious, city, expensive, little...  \n",
      "8  [boyfriend, stay, fairmont, recent, trip, fran...  \n",
      "9  [wonderful, staff, great, location, definately...  \n",
      "Shape: (38932, 6)\n",
      "Column Names ['User_ID' 'Description' 'Browser_Used' 'Device_Used' 'Is_Response'\n",
      " 'Description_lemmatized_processed']\n"
     ]
    }
   ],
   "source": [
    "#lemmatization (full version)\n",
    "\n",
    "#lemmatization using PoS tagging\n",
    "#lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "#wordnet_lemmatizer = WordNetLemmatizer()\n",
    "#from nltk.corpus import wordnet\n",
    "\n",
    "#def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "#    if nltk_tag.startswith('J'):\n",
    "#        return wordnet.ADJ\n",
    "#    elif nltk_tag.startswith('V'):\n",
    "#        return wordnet.VERB\n",
    "#    elif nltk_tag.startswith('N'):\n",
    "#        return wordnet.NOUN\n",
    "#    elif nltk_tag.startswith('R'):\n",
    "#        return wordnet.ADV\n",
    "#    else:\n",
    "#        return None\n",
    "\n",
    "#def lemmatize_sentence(sentence):\n",
    "#    #tokenize the sentence and find the POS tag for each token\n",
    "#    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "#    #tuple of (token, wordnet_tag)\n",
    "#    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "#    lemmatized_sentence = []\n",
    "#    for word, tag in wordnet_tagged:\n",
    "#        if tag is None:\n",
    "#            #if there is no available tag, append the token as is\n",
    "#            lemmatized_sentence.append(word)\n",
    "#        else:\n",
    "#            #else use the tag to lemmatize the token\n",
    "#            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "#    return \" \".join(lemmatized_sentence)\n",
    "\n",
    "\n",
    "#lemmatization (simplified version)\n",
    "def lemmatize_text(text):\n",
    "    return WordNetLemmatizer().lemmatize(text, pos='v')\n",
    "\n",
    "# and then remove stop words and words fewer than 2 characters.\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_text(token))\n",
    "    return result\n",
    "\n",
    "hotel['Description_lemmatized_processed'] = hotel['Description'].map(preprocess)\n",
    "\n",
    "print (hotel.iloc[:10,:])\n",
    "print (\"Shape:\", hotel.shape)\n",
    "print (\"Column Names\",hotel.columns.values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "651HP6PkrjfX"
   },
   "source": [
    "## Create a dictionary from the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9715,
     "status": "ok",
     "timestamp": 1678301049790,
     "user": {
      "displayName": "융융이",
      "userId": "05313301844663708595"
     },
     "user_tz": 300
    },
    "id": "2q69lgn3bRj-",
    "outputId": "9008c05d-cb09-4375-f750-86e7a60bcc95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 average\n",
      "1 better\n",
      "2 breakfast\n",
      "3 clean\n",
      "4 consider\n",
      "5 dog\n",
      "6 free\n",
      "7 fussy\n",
      "8 generally\n",
      "9 kind\n",
      "10 overnight\n",
      "37068\n"
     ]
    }
   ],
   "source": [
    "#Create a dictionary from 'Description_lemmatized_processed' containing the number of times a word appears \n",
    "#in the data set using gensim.corpora.Dictionary.\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(hotel['Description_lemmatized_processed'])\n",
    "\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break\n",
    "\n",
    "print (len(dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "crghx77-rnPS"
   },
   "source": [
    "## Remove too common & context-specific tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1678301049791,
     "user": {
      "displayName": "융융이",
      "userId": "05313301844663708595"
     },
     "user_tz": 300
    },
    "id": "13WoZSAwPhl-",
    "outputId": "6911be26-d2d1-4a32-aa90-4b8aa31e79b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "752\n"
     ]
    }
   ],
   "source": [
    "#Gensim filter_extremes.\n",
    "\n",
    "#filter_extremes(no_below=i, no_above=j, keep_n=k) where i,j,k can be integers or fractions.\n",
    "\n",
    "#Filter out tokens that appear in:\n",
    "#less than no_below documents (absolute number) or\n",
    "#more than no_above documents (fraction of total corpus size, not absolute number).\n",
    "#after (1) and (2), keep only the first keep_n most frequent tokens (or keep all if None).\n",
    "\n",
    "#Filter out tokens that appear in:\n",
    "#less than 100 documents.\n",
    "#more than 0.7 documents.\n",
    "dictionary.filter_extremes(no_below=500, no_above=0.70)\n",
    "print (len(dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NYwe7SnkruSO"
   },
   "source": [
    "## Gensim doc2bow to tranform the tokens into vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3781,
     "status": "ok",
     "timestamp": 1678301054043,
     "user": {
      "displayName": "융융이",
      "userId": "05313301844663708595"
     },
     "user_tz": 300
    },
    "id": "cL8EhTYiPhn_",
    "outputId": "c3fffd25-b9b8-417c-c8d1-3555ee9bd5a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38932\n",
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 2)]\n",
      "Word 0 (\"average\") appears 1 time.\n",
      "Word 1 (\"better\") appears 1 time.\n",
      "Word 2 (\"breakfast\") appears 1 time.\n",
      "Word 3 (\"clean\") appears 1 time.\n",
      "Word 4 (\"consider\") appears 1 time.\n",
      "Word 5 (\"free\") appears 1 time.\n",
      "Word 6 (\"generally\") appears 1 time.\n",
      "Word 7 (\"kind\") appears 1 time.\n",
      "Word 8 (\"overnight\") appears 1 time.\n",
      "Word 9 (\"price\") appears 1 time.\n",
      "Word 10 (\"right\") appears 1 time.\n",
      "Word 11 (\"smell\") appears 1 time.\n",
      "Word 12 (\"stay\") appears 2 time.\n"
     ]
    }
   ],
   "source": [
    "#Gensim doc2bow (pass the tokenized words to doc2bow and convert those to vectors.)\n",
    "\n",
    "## Caution: No further preprocessing should be done such as tokenization, lemmatization, and etc before initiating this.\n",
    "\n",
    "#For each document, check how many words and how many times those words appear.\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in hotel['Description_lemmatized_processed']]\n",
    "print (len(bow_corpus))\n",
    "print (bow_corpus[0])\n",
    "\n",
    "#pretiffy the above result to see how the 1st document in our data has been converted.\n",
    "bow_doc_0 = bow_corpus[0]\n",
    "for i in range(len(bow_doc_0)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_0[i][0], \n",
    "                                               dictionary[bow_doc_0[i][0]],bow_doc_0[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aboAp2_0r9-Z"
   },
   "source": [
    "## Applying TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 471,
     "status": "ok",
     "timestamp": 1678301080581,
     "user": {
      "displayName": "융융이",
      "userId": "05313301844663708595"
     },
     "user_tz": 300
    },
    "id": "07xPlPVIR8lp",
    "outputId": "186201c3-74b2-4af7-e1be-33d1414142ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38932\n"
     ]
    }
   ],
   "source": [
    "#create tf-idf model object using models.\n",
    "#TfidfModel on bow_corpus\n",
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "\n",
    "#Apply transformation to the entire corpus.\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "print (len(corpus_tfidf))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1678301158252,
     "user": {
      "displayName": "융융이",
      "userId": "05313301844663708595"
     },
     "user_tz": 300
    },
    "id": "L_3eHzm2hlrg",
    "outputId": "649f9f31-17b2-4a4c-b078-240a9201379e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.3400201488467968),\n",
      " (1, 0.22547077468001045),\n",
      " (2, 0.1280704209901086),\n",
      " (3, 0.09861088101941841),\n",
      " (4, 0.3316467989276336),\n",
      " (5, 0.17798059898323637),\n",
      " (6, 0.44290000747535424),\n",
      " (7, 0.3251172930602114),\n",
      " (8, 0.4304704686805295),\n",
      " (9, 0.1751490322634236),\n",
      " (10, 0.1940191700197154),\n",
      " (11, 0.3232813880437279),\n",
      " (12, 0.07588427856990787)]\n"
     ]
    }
   ],
   "source": [
    "#pretiffy the above result to see how the 1st document in our data has been converted.\n",
    "corpus_tfidf_0=corpus_tfidf[0]\n",
    "from pprint import pprint\n",
    "#for doc in corpus_tfidf:\n",
    "pprint(corpus_tfidf_0)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Env1Fp-5sINI"
   },
   "source": [
    "## Run LDA using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 92658,
     "status": "ok",
     "timestamp": 1678301602870,
     "user": {
      "displayName": "융융이",
      "userId": "05313301844663708595"
     },
     "user_tz": 300
    },
    "id": "WKhcqmfAPhqI"
   },
   "outputs": [],
   "source": [
    "#Run LDA model on the final corpus.\n",
    "\n",
    "#num_topics: the number of latent topics to be extracted from the corpus.\n",
    "#id2word: mapping from word ids (integers) to words (strings).\n",
    "# Some other parameters. See the document explanations for more details.\n",
    "\n",
    "#lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=7, id2word=dictionary, eta='auto', iterations=100,\n",
    "#                                             random_state=100, alpha='symmetric', passes=2, workers=4)\n",
    "\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=7, id2word=dictionary\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 248,
     "status": "ok",
     "timestamp": 1678301645452,
     "user": {
      "displayName": "융융이",
      "userId": "05313301844663708595"
     },
     "user_tz": 300
    },
    "id": "1tLmd6SAPhsp",
    "outputId": "b6f73690-b006-406f-8b0c-ebe2cc531901"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 francisco walk great wharf union park beach cable nice location\n",
      "Topic: 1 great pool love wonderful service staff enjoy thank beautiful stay\n",
      "Topic: 2 good free breakfast walk internet clean nice location great park\n",
      "Topic: 3 great friendly staff location comfortable service helpful good excellent clean\n",
      "Topic: 4 bathroom nice small suite shower great area large floor comfortable\n",
      "Topic: 5 york square time central subway great walk location state manhattan\n",
      "Topic: 6 check tell desk night go call leave say like work\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1, num_words=10):\n",
    "    #print out topic numbers and keywords.\n",
    "#    print('Topic: {} Word: {}'.format(idx, topic))\n",
    "    \n",
    "    #print out keywords only (without probability)\n",
    "    key_words_only = \" \".join(re.findall(\"[a-zA-Z]+\", topic))\n",
    "    print ('Topic:',idx,key_words_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T7ASDR-1sK3K"
   },
   "source": [
    "## Finding optimal number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "executionInfo": {
     "elapsed": 326430,
     "status": "error",
     "timestamp": 1678302596700,
     "user": {
      "displayName": "융융이",
      "userId": "05313301844663708595"
     },
     "user_tz": 300
    },
    "id": "OMWormJ1Phuv",
    "outputId": "02c92669-2246-4830-8d85-5f0622e3658a"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-f08169e0db42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoherence_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Num Topics\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Coherence score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "def calculate_coherence_values(dictionary, corpus, texts, limit, start=4, step=2):\n",
    "    '''\n",
    "    Compute c_v coherence for various number of topics to find the OPTIMAL NUMBER OF TOPICS.\n",
    "\n",
    "    Parameters:\n",
    "              dictionary : Gensim dictionary\n",
    "              corpus : Gensim corpus\n",
    "              texts : List of input texts\n",
    "              limit : Max num of topics\n",
    "              start: starting number of topics\n",
    "              step=n : increase the number by n\n",
    "    '''\n",
    "\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model =  gensim.models.LdaMulticore(corpus_tfidf, num_topics=num_topics, id2word=dictionary\n",
    "                                             )\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=hotel['Description_lemmatized_processed'], dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "    return model_list, coherence_values\n",
    "\n",
    "# Can take a while to see the outputs.\n",
    "model_list, coherence_values = calculate_coherence_values(dictionary=dictionary, corpus=corpus_tfidf, texts=hotel['Description_lemmatized_processed'], start=4, limit=10, step=2)\n",
    "\n",
    "# Show graph\n",
    "limit=10; start=4; step=2;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bgof4e3ZsN7G"
   },
   "source": [
    "## Find dominant topics for each doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "aborted",
     "timestamp": 1678302596701,
     "user": {
      "displayName": "융융이",
      "userId": "05313301844663708595"
     },
     "user_tz": 300
    },
    "id": "3RRXqaGkPhxF"
   },
   "outputs": [],
   "source": [
    "#Find the dominant topic in each document\n",
    "list_in_list=[]\n",
    "\n",
    "#for the first 4 documents.\n",
    "doc_topics = lda_model_tfidf.get_document_topics(corpus_tfidf[:5])\n",
    "print('document topics: ', doc_topics)\n",
    "\n",
    "for each_doc in doc_topics:\n",
    "    \n",
    "    #get the assigned topics and list them in terms of their highest to lowest probability.\n",
    "    top_topic=sorted(each_doc, key=lambda x: x[1], reverse=True)[0]\n",
    "\n",
    "    #If you want to see the 2nd dominant topic:\n",
    "    #top_topic_2=sorted(each_doc, key=lambda x: x[1], reverse=True)[1]\n",
    "    \n",
    "    print (\"highest\",top_topic)\n",
    "    \n",
    "    #select topic number & its probability.\n",
    "    top_topic_num=top_topic[0]\n",
    "    top_topic_prob=top_topic[1]\n",
    "\n",
    "    #make a dataframe and fill out within each column.        \n",
    "    list=[top_topic_num,top_topic_prob]\n",
    "    list_in_list.extend([list])\n",
    "    #print (list_in_list)\n",
    "#create pandas dataframe with column names.\n",
    "df = pd.DataFrame(list_in_list, columns = ['Dominant Topic', 'Probability']) \n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "print (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1sK3J_zSPhzb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "axJXTZCTPh18"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dmxkIx6pbcho"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNq751QZyHu0M7n38xKF/fS",
   "provenance": [
    {
     "file_id": "18ABo6nZcd4oAtvjoFeCCC4iCbmIrr26U",
     "timestamp": 1648664619231
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
