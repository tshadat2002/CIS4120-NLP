{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5hcX938dVz5t"
   },
   "source": [
    "## Week 3 Pre-processing Raw Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "kqfvUSmmYcMD"
   },
   "outputs": [],
   "source": [
    "nlp=\"\"\"\n",
    "Natural language processing (NLP) refers to the branch of computer science—and more specifically, the branch of \n",
    "artificial intelligence or AI—concerned with giving computers the ability to understand text and spoken words in much \n",
    "the same way human beings can. \n",
    "There’s a good chance you’ve interacted with NLP in the form of voice-operated GPS systems.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GL3rUKqvWBAB"
   },
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "aYkykS_7YcQL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'language', 'processing', '(', 'NLP', ')', 'refers', 'to', 'the', 'branch', 'of', 'computer', 'science—and', 'more', 'specifically', ',', 'the', 'branch', 'of', 'artificial', 'intelligence', 'or', 'AI—concerned', 'with', 'giving', 'computers', 'the', 'ability', 'to', 'understand', 'text', 'and', 'spoken', 'words', 'in', 'much', 'the', 'same', 'way', 'human', 'beings', 'can', '.', 'There', '’', 's', 'a', 'good', 'chance', 'you', '’', 've', 'interacted', 'with', 'NLP', 'in', 'the', 'form', 'of', 'voice-operated', 'GPS', 'systems', '.']\n",
      "63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/taohidshadat/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#tokenize the string nlp.\n",
    "#seprating tokens using punctuations and spaces.\n",
    "#commas, parentheses, periods are treated as tokens.\n",
    "#import nltk\n",
    "#nltk.download('punkt')\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "nlp_tokens=word_tokenize(nlp)\n",
    "print (nlp_tokens)\n",
    "print (len(nlp_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "wh4dxccqYcT6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'language', 'processing', '(', 'NLP', ')', 'refers', 'to', 'the', 'branch', 'of', 'computer', 'science', '—', 'and', 'more', 'specifically', ',', 'the', 'branch', 'of', 'artificial', 'intelligence', 'or', 'AI', '—', 'concerned', 'with', 'giving', 'computers', 'the', 'ability', 'to', 'understand', 'text', 'and', 'spoken', 'words', 'in', 'much', 'the', 'same', 'way', 'human', 'beings', 'can', '.', 'There', '’', 's', 'a', 'good', 'chance', 'you', '’', 've', 'interacted', 'with', 'NLP', 'in', 'the', 'form', 'of', 'voice', '-', 'operated', 'GPS', 'systems', '.']\n",
      "69\n"
     ]
    }
   ],
   "source": [
    "#seperates the punctuations from the words.\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "  \n",
    "tokenizer2 = WordPunctTokenizer()\n",
    "nlp_tokens_3=tokenizer2.tokenize(nlp)\n",
    "print (nlp_tokens_3)\n",
    "print (len(nlp_tokens_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "VhzRD-3NYcWm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'language', 'processing', 'NLP', 'refers', 'to', 'the', 'branch', 'of', 'computer', 'science', 'and', 'more', 'specifically', 'the', 'branch', 'of', 'artificial', 'intelligence', 'or', 'AI', 'concerned', 'with', 'giving', 'computers', 'the', 'ability', 'to', 'understand', 'text', 'and', 'spoken', 'words', 'in', 'much', 'the', 'same', 'way', 'human', 'beings', 'can', 'There', 's', 'a', 'good', 'chance', 'you', 've', 'interacted', 'with', 'NLP', 'in', 'the', 'form', 'of', 'voice', 'operated', 'GPS', 'systems']\n",
      "59\n"
     ]
    }
   ],
   "source": [
    "nlp=\"\"\"\n",
    "Natural language processing (NLP) refers to the branch of computer science—and more specifically, the branch of \n",
    "artificial intelligence or AI—concerned with giving computers the ability to understand text and spoken words in much \n",
    "the same way human beings can. \n",
    "There’s a good chance you’ve interacted with NLP in the form of voice-operated GPS systems !!!!.\n",
    "\"\"\"\n",
    "#all the punctuations have been removed.\n",
    "\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "\n",
    "nlp_tokens_4=regexp_tokenize(nlp, \"[\\w]+\")  #match words\n",
    "print (nlp_tokens_4)\n",
    "print (len(nlp_tokens_4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kKja3Je6WOVD"
   },
   "source": [
    "### Expand Contraction Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "gXfe9GLaYcgV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting contractions\n",
      "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
      "Collecting textsearch>=0.0.21\n",
      "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting anyascii\n",
      "  Downloading anyascii-0.3.1-py3-none-any.whl (287 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.5/287.5 kB\u001b[0m \u001b[31m415.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pyahocorasick\n",
      "  Downloading pyahocorasick-2.0.0-cp38-cp38-macosx_10_9_x86_64.whl (37 kB)\n",
      "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
      "Successfully installed anyascii-0.3.1 contractions-0.1.73 pyahocorasick-2.0.0 textsearch-0.0.24\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "R0CmK-GRYcjM"
   },
   "outputs": [],
   "source": [
    "nlp=\"\"\"\n",
    "Natural language processing (NLP) refers to the branch of computer science—and more specifically, the branch of \n",
    "artificial intelligence or AI—concerned with giving computers the ability to understand text and spoken words in much \n",
    "the same way human beings can. \n",
    "There’s a good chance you’ve interacted with NLP in the form of voice-operated GPS systems.\n",
    "\n",
    "I'll, there's, isn't, we're, we've\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "DamjYG_lYcld"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Natural language processing (NLP) refers to the branch of computer science—and more specifically, the branch of \n",
      "artificial intelligence or AI—concerned with giving computers the ability to understand text and spoken words in much \n",
      "the same way human beings can. \n",
      "There is a good chance you have interacted with NLP in the form of voice-operated GPS systems.\n",
      "\n",
      "I will, there is, is not, we are, we have\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import contractions \n",
    "cont_nlp=contractions.fix(nlp)\n",
    "print (cont_nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "b5SWI44HYcnf"
   },
   "outputs": [],
   "source": [
    "#tokenize\n",
    "nlp_tokens=word_tokenize(cont_nlp)\n",
    "#and then remove punctuations\n",
    "remove_punc=[word for word in nlp_tokens if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "HBVha7qE2Dbt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'language', 'processing', 'NLP', 'refers', 'to', 'the', 'branch', 'of', 'computer', 'more', 'specifically', 'the', 'branch', 'of', 'artificial', 'intelligence', 'or', 'with', 'giving', 'computers', 'the', 'ability', 'to', 'understand', 'text', 'and', 'spoken', 'words', 'in', 'much', 'the', 'same', 'way', 'human', 'beings', 'can', 'There', 'is', 'a', 'good', 'chance', 'you', 'have', 'interacted', 'with', 'NLP', 'in', 'the', 'form', 'of', 'GPS', 'systems', 'I', 'will', 'there', 'is', 'is', 'not', 'we', 'are', 'we', 'have']\n",
      "63\n"
     ]
    }
   ],
   "source": [
    "print (remove_punc)\n",
    "print (len(remove_punc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LG5SmtPiWdIk"
   },
   "source": [
    "### Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ZZqzdiMJ2DeS"
   },
   "outputs": [],
   "source": [
    "NLP=\"\"\"\n",
    "Natural language processing (NLP) refers to the branch of computer science—and more specifically, the branch of \n",
    "artificial intelligence or AI—concerned with giving computers the ability to understand text and spoken words in much \n",
    "the same way human beings can. \n",
    "There’s a good chance you’ve interacted with NLP in the form of voice-operated GPS systems.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "-U2S_3Yh2DgP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/taohidshadat/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk_stop_words=stopwords.words('english')\n",
    "print (nltk_stop_words)\n",
    "print (len(nltk_stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "yrMj4net2DiZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'language', 'processing', '(', 'NLP', ')', 'refers', 'to', 'the', 'branch', 'of', 'computer', 'science—and', 'more', 'specifically', ',', 'the', 'branch', 'of', 'artificial', 'intelligence', 'or', 'AI—concerned', 'with', 'giving', 'computers', 'the', 'ability', 'to', 'understand', 'text', 'and', 'spoken', 'words', 'in', 'much', 'the', 'same', 'way', 'human', 'beings', 'can', '.', 'There', '’', 's', 'a', 'good', 'chance', 'you', '’', 've', 'interacted', 'with', 'NLP', 'in', 'the', 'form', 'of', 'voice-operated', 'GPS', 'systems', '.']\n",
      "63\n",
      "\n",
      "['Natural', 'language', 'processing', '(', 'NLP', ')', 'refers', 'branch', 'computer', 'science—and', 'specifically', ',', 'branch', 'artificial', 'intelligence', 'AI—concerned', 'giving', 'computers', 'ability', 'understand', 'text', 'spoken', 'words', 'much', 'way', 'human', 'beings', '.', 'There', '’', 'good', 'chance', '’', 'interacted', 'NLP', 'form', 'voice-operated', 'GPS', 'systems', '.']\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "nlp_tokens = word_tokenize(NLP)\n",
    "\n",
    "#for each word that is not included in the stop-word set, save it.\n",
    "nlp_tokens_no_sw = [w for w in nlp_tokens if w not in nltk_stop_words]\n",
    "\n",
    "print(nlp_tokens)\n",
    "print(len(nlp_tokens))\n",
    "print ()\n",
    "print(nlp_tokens_no_sw)\n",
    "print(len(nlp_tokens_no_sw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "GHWQEZVj2DlW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'language', 'processing', 'NLP', 'refers', 'branch', 'computer', 'specifically', 'branch', 'artificial', 'intelligence', 'giving', 'computers', 'ability', 'understand', 'text', 'spoken', 'words', 'much', 'way', 'human', 'beings', 'There', 'good', 'chance', 'interacted', 'NLP', 'form', 'GPS', 'systems']\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "#After revmoing stop-words, remove punctuations.\n",
    "\n",
    "nlp_tokens_no_sw_punc=[word for word in nlp_tokens_no_sw if word.isalpha()]\n",
    "\n",
    "print (nlp_tokens_no_sw_punc)\n",
    "print (len(nlp_tokens_no_sw_punc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cY6lLzRKoOxi"
   },
   "source": [
    "### Remove Stopwords Using Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "B1Gya_clEJwF"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name dataclass_transform",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/7z/vz6nhrcd5v732wwdb25fz_rh0000gn/T/ipykernel_5782/4205716738.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0men\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEnglish\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnglish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# set library-specific custom warning handling before doing anything else\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msetup_default_warnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0msetup_default_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/spacy/errors.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLiteral\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mErrorsWithCodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/spacy/compat.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\"\"\"Helpers for Python and platform compatibility.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mthinc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcopy_array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/thinc/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mabout\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/thinc/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcatalogue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mconfection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mconfection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConfigValidationError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPromise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVARIABLE_RE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDecorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/confection/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mconfigparser\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParsingError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpydantic\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValidationError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExtra\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpydantic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModelMetaclass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpydantic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfields\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModelField\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pydantic/__init__.cpython-38-darwin.so\u001b[0m in \u001b[0;36minit pydantic.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pydantic/dataclasses.cpython-38-darwin.so\u001b[0m in \u001b[0;36minit pydantic.dataclasses\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name dataclass_transform"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "nlp = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "B6uUtH5JEJyZ"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/7z/vz6nhrcd5v732wwdb25fz_rh0000gn/T/ipykernel_5782/3715306817.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmy_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNLP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmy_doc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "my_doc = nlp(NLP)\n",
    "print (my_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ukb6mj_aEJ0m"
   },
   "outputs": [],
   "source": [
    "#tokenize the string.\n",
    "token_list = []\n",
    "for token in my_doc:\n",
    "    token_list.append(token.text)\n",
    "print (token_list)\n",
    "print (len(token_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9OM9trv2Euzd"
   },
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vcdq9FnpEu3z"
   },
   "outputs": [],
   "source": [
    "after_sw_removal =[] \n",
    "\n",
    "for word in token_list:\n",
    "    a = nlp.vocab[word]\n",
    "    # if a token is not a stop-word, append it in the empty list.\n",
    "    if a.is_stop == False:\n",
    "        after_sw_removal.append(word) \n",
    "\n",
    "print(after_sw_removal)\n",
    "print (len(after_sw_removal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h4PIc514oz_y"
   },
   "source": [
    "### Remove Stopwords Using Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "CliMxMTyEu6B"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.parsing.preprocessing import STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "XAdStawfEu8T"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'thereby', 'whatever', 'doing', 'her', 'out', 'thru', 'eleven', 'formerly', 'empty', 'serious', 'mill', 'where', 'besides', 'they', 'take', 'up', 'would', 'until', 'thus', 'few', 'are', 'she', 'sincere', 'rather', 'several', 'hundred', 'nothing', 'something', 'front', 'hence', 'others', 'describe', 'ours', 'per', 'nobody', 'another', 'anything', 'has', 'other', 'so', 'amount', 'same', 'without', 'bottom', 'beside', 'except', 'while', 'should', 'otherwise', 'first', 'do', 'anyhow', 'towards', 'everything', 'around', 'we', 'as', 'move', 'amongst', 'if', 'that', 'cannot', 'couldnt', 'side', 'must', 'behind', 'whereafter', 'back', 'whether', 'toward', 'thereupon', 'have', 'call', 'himself', 'but', 'sometime', 'since', 'co', 'unless', 'our', 'whereas', 'latterly', 'many', 'me', 'due', 'and', 'de', 'anywhere', 'become', 'each', 'it', 'neither', 'perhaps', 'across', 'cry', 'anyone', 'there', 'fire', 'your', 'you', 'used', 'whereby', 'anyway', 'herself', 'at', 'kg', 'becomes', 'last', 'them', 'all', 'against', 'does', 'already', 'below', 'km', 'further', 'hasnt', 'find', 'four', 'cant', 'these', 'enough', 'almost', 'themselves', 'again', 'full', 'meanwhile', 'seemed', 'both', 'six', 'he', 'yourself', 'him', 'how', 'herein', 'during', 'name', 'someone', 'sixty', 'never', 'thick', 'noone', 'than', 'keep', 'whom', 'quite', 'always', 'twenty', 'once', 'bill', 'latter', 'did', 'on', 'some', 'ourselves', 'found', 'into', 'top', 'ltd', 'along', 'former', 'his', 'hers', 'whose', 'although', 'nowhere', 'be', 'well', 'eight', 'over', 'whoever', 'is', 'becoming', 'less', 'fifty', 'thereafter', 'myself', 'whole', 'one', 'seeming', 'five', 'in', 'fill', 'ie', 'various', 'together', 'system', 'made', 'really', 'either', 'only', 'or', 'too', 'everyone', 'from', 'why', 'etc', 'therein', 'please', 'somewhere', 'often', 'show', 'might', 'which', 'who', 'done', 'more', 'go', 'eg', 'indeed', 'may', 'hereafter', 'using', 'somehow', 'interest', 'those', 'through', 'became', 'con', 'yet', 'upon', 'because', 'twelve', 'been', 'am', 'their', 'say', 'just', 'this', 'hereby', 're', 'before', 'none', 'next', 'a', 'nevertheless', 'nor', 'the', 'amoungst', 'by', 'itself', 'not', 'yours', 'could', 'though', 'moreover', 'above', 'mostly', 'wherever', 'then', 'very', 'no', 'after', 'between', 'didn', 'detail', 'elsewhere', 'whence', 'whereupon', 'within', 'such', 'here', 'with', 'nine', 'most', 'sometimes', 'yourselves', 'still', 'regarding', 'i', 'however', 'seem', 'whither', 'wherein', 'throughout', 'else', 'ten', 'of', 'seems', 'an', 'via', 'to', 'doesn', 'see', 'also', 'its', 'when', 'my', 'therefore', 'had', 'alone', 'thin', 'don', 'afterwards', 'hereupon', 'every', 'us', 'off', 'what', 'ever', 'beforehand', 'mine', 'three', 'give', 'inc', 'make', 'third', 'can', 'computer', 'put', 'fifteen', 'everywhere', 'was', 'about', 'own', 'onto', 'down', 'beyond', 'any', 'even', 'namely', 'under', 'will', 'were', 'part', 'get', 'now', 'for', 'two', 'whenever', 'least', 'being', 'thence', 'forty', 'much', 'un', 'among'})\n",
      "337\n",
      "['Natural', 'language', 'processing', '(', 'NLP', ')', 'refers', 'to', 'the', 'branch', 'of', 'computer', 'science—and', 'more', 'specifically', ',', 'the', 'branch', 'of', 'artificial', 'intelligence', 'or', 'AI—concerned', 'with', 'giving', 'computers', 'the', 'ability', 'to', 'understand', 'text', 'and', 'spoken', 'words', 'in', 'much', 'the', 'same', 'way', 'human', 'beings', 'can', '.', 'There', '’', 's', 'a', 'good', 'chance', 'you', '’', 've', 'interacted', 'with', 'NLP', 'in', 'the', 'form', 'of', 'voice-operated', 'GPS', 'systems', '.']\n",
      "63\n",
      "['Natural', 'language', 'processing', '(', 'NLP', ')', 'refers', 'branch', 'science—and', 'specifically', ',', 'branch', 'artificial', 'intelligence', 'AI—concerned', 'giving', 'computers', 'ability', 'understand', 'text', 'spoken', 'words', 'way', 'human', 'beings', '.', 'There', '’', 's', 'good', 'chance', '’', 've', 'interacted', 'NLP', 'form', 'voice-operated', 'systems', '.']\n",
      "39\n"
     ]
    }
   ],
   "source": [
    "stopwords_gensim = STOPWORDS\n",
    "print (stopwords_gensim)\n",
    "print (len(stopwords_gensim))\n",
    "\n",
    "nlp_tokens = word_tokenize(NLP)\n",
    "print (nlp_tokens)\n",
    "print (len(nlp_tokens))\n",
    "\n",
    "#Add a particular token that you want to remove from the tokenized list.\n",
    "add_on=['GPS']\n",
    "\n",
    "resultss_=[word for word in nlp_tokens if word not in stopwords_gensim and word not in add_on ] \n",
    "print (resultss_)\n",
    "print (len(resultss_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7P22OwDypR7U"
   },
   "source": [
    "### Applying Lower-case Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "BgDqyIapXr26"
   },
   "outputs": [],
   "source": [
    "#convert elements to lowercase.\n",
    "#lower_nlp=[w.lower() for w in nlp_tokens]\n",
    "#print (lower_nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "J9sV_CpPEu-o"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'language', 'processing', '(', 'NLP', ')', 'refers', 'to', 'the', 'branch', 'of', 'computer', 'science—and', 'more', 'specifically', ',', 'the', 'branch', 'of', 'artificial', 'intelligence', 'or', 'AI—concerned', 'with', 'giving', 'computers', 'the', 'ability', 'to', 'understand', 'text', 'and', 'spoken', 'words', 'in', 'much', 'the', 'same', 'way', 'human', 'beings', 'can', '.', 'There', '’', 's', 'a', 'good', 'chance', 'you', '’', 've', 'interacted', 'with', 'NLP', 'in', 'the', 'form', 'of', 'voice-operated', 'GPS', 'systems', '.']\n",
      "63\n",
      "['natural', 'language', 'processing', '(', 'nlp', ')', 'refers', 'branch', 'science—and', 'specifically', ',', 'branch', 'artificial', 'intelligence', 'ai—concerned', 'giving', 'computers', 'ability', 'understand', 'text', 'spoken', 'words', 'way', 'human', 'beings', '.', 'there', '’', 's', 'good', 'chance', '’', 've', 'interacted', 'nlp', 'form', 'voice-operated', 'systems', '.']\n",
      "39\n"
     ]
    }
   ],
   "source": [
    "#apply lowercase at the same time.\n",
    "stopwords_gensim = STOPWORDS\n",
    "\n",
    "#tokenize the string first.\n",
    "nlp_tokens = word_tokenize(NLP)\n",
    "print (nlp_tokens)\n",
    "print (len(nlp_tokens))\n",
    "\n",
    "#Add a particular token that you want to remove from the tokenized list.\n",
    "add_on=['GPS']\n",
    "\n",
    "#simply add lowercase function at the front (3 conditions in this list).\n",
    "resultss_=[word.lower() for word in nlp_tokens if word not in stopwords_gensim and word not in add_on ]\n",
    "print (resultss_)\n",
    "print (len(resultss_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SL4Xu8PAcAAA"
   },
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "g_R-AoV12Dpj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "languag\n",
      "leav\n",
      "fairli\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "ps = nltk.PorterStemmer()\n",
    "print (ps.stem('language'))\n",
    "print (ps.stem('leave'))\n",
    "print (ps.stem('fairly'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "764UQUHzWm5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "languag\n",
      "leav\n",
      "fair\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "snow = nltk.stem.SnowballStemmer('english')\n",
    "print (snow.stem('language'))\n",
    "print (snow.stem('leave'))\n",
    "print (snow.stem('fairly'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "thkKtoHRi3tH"
   },
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "YaMzPo3jcDL-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('nlp', 'JJ'), ('refers', 'NNS'), ('branch', 'VBP'), ('specifically', 'RB'), ('branch', 'JJ'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('giving', 'VBG'), ('computers', 'NNS'), ('ability', 'NN'), ('understand', 'VBP'), ('text', 'NN'), ('spoken', 'VBN'), ('words', 'NNS'), ('way', 'NN'), ('human', 'JJ'), ('beings', 'NNS'), ('there', 'RB'), ('s', 'RB'), ('good', 'JJ'), ('chance', 'NN'), ('ve', 'NN'), ('interacted', 'VBD'), ('nlp', 'JJ'), ('form', 'NN'), ('systems', 'NNS')]\n"
     ]
    }
   ],
   "source": [
    "#PoS Tagging. \n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "#lower-casing, removing stop-words and a particular token, punctuations\n",
    "resultss_=[word.lower() for word in nlp_tokens if word not in stopwords_gensim and word not in add_on and word.isalpha()]\n",
    "\n",
    "pos_tagging=nltk.pos_tag(resultss_)\n",
    "print (pos_tagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Z6K4aGbihUsY"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/taohidshadat/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/taohidshadat/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'processing', 'nlp', 'refers', 'branch', 'specifically', 'branch', 'artificial', 'intelligence', 'giving', 'computers', 'ability', 'understand', 'text', 'spoken', 'words', 'way', 'human', 'beings', 'there', 's', 'good', 'chance', 've', 'interacted', 'nlp', 'form', 'systems']\n",
      "['natural', 'language', 'process', 'nlp', 'refer', 'branch', 'specifically', 'branch', 'artificial', 'intelligence', 'give', 'computer', 'ability', 'understand', 'text', 'speak', 'word', 'way', 'human', 'be', 'there', 's', 'good', 'chance', 've', 'interact', 'nlp', 'form', 'system']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "lemma_eg=[WordNetLemmatizer().lemmatize(w, pos='v') for w in resultss_]     #verb\n",
    "lemma_eg=[WordNetLemmatizer().lemmatize(w, pos='n') for w in lemma_eg]      #noun\n",
    "lemma_eg=[WordNetLemmatizer().lemmatize(w, pos='a') for w in lemma_eg]      #adjective\n",
    "\n",
    "print (resultss_)\n",
    "print (lemma_eg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GvFxOBGgnR_s"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
