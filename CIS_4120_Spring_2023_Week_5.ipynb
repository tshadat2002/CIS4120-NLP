{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFhIktmFmU_P"
      },
      "source": [
        "# Week 5 Classifications Using Text Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2A-H8PS6cn3"
      },
      "source": [
        "###connect Colab to your Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5ZeYgeZmUYs"
      },
      "outputs": [],
      "source": [
        "#connect Colab to your Google Drive.\n",
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzh37UbtfV4O"
      },
      "source": [
        "# Part 1: News Categorization using Multinomial Naive Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5HMVkh7fzIe"
      },
      "source": [
        "Classification is the task of choosing the correct class label for a given input. In basic classification tasks, each input is considered in isolation from all other inputs, and the set of labels is defined in advance. Some examples of classification tasks are:\n",
        "\n",
        "* Deciding whether an email is spam or not.\n",
        "*  **Deciding what the topic of a news article is, from a fixed list of topic areas such as \"sports,\" \"technology,\" and \"politics.\"**\n",
        "* Deciding whether a given occurrence of the word bank is used to refer to a river bank, a financial institution, the act of tilting to the side, or the act of depositing something in a financial institution.\n",
        "\n",
        "A classifier is called supervised if it is built based on **training data** containing the correct label for each input. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fuAR7KX8fYh5"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image\n",
        "Image(url=\"http://www.nltk.org/images/supervised-classification.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFYd1fW-gEZY"
      },
      "source": [
        "(a) During training, we have a set of input cases, for which we know their correct label. \n",
        "Then we take each input and we extract a set of _features_, which capture the basic information \n",
        "about each input. \n",
        "Pairs of feature sets and labels are fed into the machine learning algorithm to generate a model. \n",
        "\n",
        "(b) During prediction, we need to classify input for which we do not have the correct label. \n",
        "For that, we extract the  same set of features from the input. we feed these features into the model, \n",
        "which generates predicted labels.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zoqtlw-9gNcU"
      },
      "source": [
        "The objective of this notebook is to show how to use Multinomial Naive Bayes method to classify news according to some predefined classes. (adapted from Andres Soto Villaverde)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2JovNNbgQFc"
      },
      "source": [
        "The News Aggregator Data Set comes from the UCI Machine Learning Repository. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2wR0sVRgRFa"
      },
      "source": [
        "* Lichman, M. (2013). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3N6p4z-kgUgO"
      },
      "source": [
        "### Data Source:\n",
        "This specific dataset can be found in the UCI ML Repository at this URL: http://archive.ics.uci.edu/ml/datasets/News+Aggregator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehX2iu-3gUvI"
      },
      "source": [
        "### About Data:\n",
        "This dataset contains headlines, URLs, and categories for 422,937 news stories collected by a web aggregator between March 10th, 2014 and August 10th, 2014. News categories in this dataset are labelled:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fctAMkaAgo3U"
      },
      "source": [
        "* b: business; \n",
        "* t: science and technology; \n",
        "* e: entertainment; and \n",
        "* m: health. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjOGPhyaiukH"
      },
      "source": [
        "### Our Goal:\n",
        "Using Multinomial Naive Bayes method, we will try to predict the category (business, entertainment, etc.) of a news article given its headline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-i4tnhmYkx-u"
      },
      "source": [
        "## Naive Bayes Classifier (Optional)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkSubVackyMQ"
      },
      "source": [
        "The naive bayes classifier is a probabilistic classifier that, given a set of features, tries to find the class with the highest probability. It is based on applying Bayes' theorem and is called naive because of its strong independence assumption between features. This means that the absence or presence of each feature is assumed to be independent of each other. We compute the posterior probability of a class as the product of the prior probability of a class and the joint probability of all features given that class:\n",
        "\n",
        "$$ P(y|x_1,\\ldots,x_n) \\propto P(y) \\prod^n_{i=1} P(x_i|y) $$\n",
        "\n",
        "Classification is based on the *maximum a posteriori* or MAP descision rule which simply picks the class (or author in our case) that is most probable:\n",
        "\n",
        "$$ predict(x_1, \\ldots, x_n) = \\arg\\max_y P(y) \\prod^n_{i=1} P(x_i|y) $$\n",
        "\n",
        "If you're unfamiliar with reading formulas, this might all seem quite daunting. To better understand what is going on, let's work out a small example. Say we have a small corpus of five very short books of which the author of the fifth book is unknown. The total vocabulary $V$ is 10 words long. For each book we store how often each word $w_i \\in V$ occurs:\n",
        "\n",
        "\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>the</th>\n",
        "      <th>poetry</th>\n",
        "      <th>society</th>\n",
        "      <th>america</th>\n",
        "      <th>realism</th>\n",
        "      <th>a</th>\n",
        "      <th>harry</th>\n",
        "      <th>magic</th>\n",
        "      <th>health</th>\n",
        "      <th>system</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>David Foster Wallace</th>\n",
        "      <td>  6</td>\n",
        "      <td>  4</td>\n",
        "      <td> 3</td>\n",
        "      <td> 3</td>\n",
        "      <td> 0</td>\n",
        "      <td> 10</td>\n",
        "      <td>  0</td>\n",
        "      <td>  0</td>\n",
        "      <td>  1</td>\n",
        "      <td> 4</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>David Foster Wallace</th>\n",
        "      <td>  8</td>\n",
        "      <td>  1</td>\n",
        "      <td> 2</td>\n",
        "      <td> 4</td>\n",
        "      <td> 0</td>\n",
        "      <td>  7</td>\n",
        "      <td>  0</td>\n",
        "      <td>  0</td>\n",
        "      <td> 10</td>\n",
        "      <td> 3</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>Walt Whitman</th>\n",
        "      <td> 12</td>\n",
        "      <td> 10</td>\n",
        "      <td> 1</td>\n",
        "      <td> 8</td>\n",
        "      <td> 0</td>\n",
        "      <td>  4</td>\n",
        "      <td>  0</td>\n",
        "      <td>  0</td>\n",
        "      <td>  0</td>\n",
        "      <td> 4</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>J.K. Rowling</th>\n",
        "      <td>  8</td>\n",
        "      <td>  0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td>  15</td>\n",
        "      <td> 12</td>\n",
        "      <td> 10</td>\n",
        "      <td>  0</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>?</th>\n",
        "      <td>  7</td>\n",
        "      <td>  4</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td>  12</td>\n",
        "      <td> 6</td>\n",
        "      <td> 8</td>\n",
        "      <td>  3</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "\n",
        "What is the probability of $P(y=\\textrm{Walt Whitman}|x = [12, 10, 1, 8, 0, 4, 0, 0, 0, 4])$? And what is the probability of $P(y=\\textrm{J.K. Rowling}|x = [7, 4, 0, 0, 0, 12, 6, 8, 3, 0])$?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ee3d6OBXkyVh"
      },
      "source": [
        "The probability of a word like *the* given some author is computed by dividing the number of occurences of that word by the total number of words for that author. In the case of Walt Whitman, the probability of the word *poetry* is:\n",
        "\n",
        "$$\n",
        "\\begin{array}{lll}\n",
        "P(x_i=\\textrm{poetry}|y=\\textrm{Walt Whitman}) & = & \\frac{10}{12 + 10 + 1 + 8 + 0 + 4 + 0 + 0 + 0 + 4}\\\\\n",
        "                                         & = & \\frac{10}{39} \\\\\n",
        "                                         & = & 0.256 \\\\\n",
        "\\end{array}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xgnwY1alBMC"
      },
      "source": [
        "The posterior probability of a class computes the joint probability of all features given that class. This means that for each nonzero word $w_1, w_2, \\ldots, w_n$ in our unknown book, we compute the probability of that word given a particular author $y$: $P(w_i|y)$. We then take the product (joint probability) of these individual words, which multiplied by the prior probability of the author, provides us with the posterior probability. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tl7TxCjZlBXT"
      },
      "source": [
        "Let's begin importing the Pandas (Python Data Analysis Library) module. The import statement is the most common way to gain access to the code in another module. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJVAxJdbfYtC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vI5rDwDRgDgh"
      },
      "outputs": [],
      "source": [
        "#import csv file and put it into a Pandas dataframe.\n",
        "news=pd.read_csv('/content/gdrive/My Drive/CIS NLP Data Sets/newsCorpora.csv', sep=\"\\t\")\n",
        "\n",
        "#assign column names.\n",
        "news.columns=[\"ID\",\"TITLE\",\"URL\",\"PUBLISHER\",\"CATEGORY\",\"STORY\",\"HOSTNAME\",\"TIMESTAMP\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UeeDqnJZmxLT"
      },
      "outputs": [],
      "source": [
        "news.head()\n",
        "#print (data.iloc[:10,:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-FOpzPSmxUB"
      },
      "outputs": [],
      "source": [
        "#How many columns and rows?\n",
        "print (\"Shape:\", news.shape)\n",
        "\n",
        "#Column names?\n",
        "print (\"Column Names\",news.columns.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrVvot27nOv9"
      },
      "outputs": [],
      "source": [
        "categories = news['CATEGORY']\n",
        "titles = news['TITLE']\n",
        "\n",
        "\n",
        "#What values exist within category?\n",
        "labels = list(set(categories))\n",
        "print('possible categories',labels)\n",
        "\n",
        "#b: business\n",
        "#t: science & technology\n",
        "#e: entertainment\n",
        "#m: health\n",
        "\n",
        "#cross-tabulation of category column.\n",
        "num_freq=news['CATEGORY'].value_counts()\n",
        "print (num_freq)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cfo8yRhlnO3B"
      },
      "outputs": [],
      "source": [
        "#Categories are literal labels, but it is better for machine learning algorithms just to work with numbers.\n",
        "#so we will encode them using LabelEncoder, which encode labels with value between 0 and n_classes-1.\n",
        "\n",
        "#Class LabelEncoder allows to encode labels with values between 0 and n_classes-1.\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "encoder = LabelEncoder()                       #Encode labels with value between 0 and n_classes-1.\n",
        "ncategories = encoder.fit_transform(categories)\n",
        "\n",
        "#What values exist within category?\n",
        "labels_category = list(set(ncategories))\n",
        "print('possible encoded categories',labels_category)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maiLXSP7qSOo"
      },
      "source": [
        "- Now we should split our data into two sets:\n",
        "1. a training set (70%) used to discover potentially predictive relationships, and\n",
        "2. a test set (30%) used to evaluate whether the discovered relationships hold and to assess the strength and utility of a predictive relationship."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ls2xwQKRqhh0"
      },
      "outputs": [],
      "source": [
        "#Samples should be first shuffled and then split into a pair of train and test sets. \n",
        "#Make sure you shuffle your training data before fitting the model.\n",
        "\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "titles, ncategories = shuffle(titles, ncategories, random_state=0) #shuffle my matrices randomly\n",
        "\n",
        "N = len(titles)\n",
        "Ntrain = int(N * 0.7) #we use 70% of the data as training set\n",
        "\n",
        "#You can use train_test_split function for this step too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irE9Cgl2qhyp"
      },
      "outputs": [],
      "source": [
        "X_train = titles[:Ntrain]\n",
        "print('X_train.shape',X_train.shape)\n",
        "y_train = ncategories[:Ntrain]\n",
        "print('y_train.shape',y_train.shape)\n",
        "X_test = titles[Ntrain:]\n",
        "print('X_test.shape',X_test.shape)\n",
        "y_test = ncategories[Ntrain:]\n",
        "print('y_test.shape',y_test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHgl8y9qnO82"
      },
      "outputs": [],
      "source": [
        "print (126726/(126726+295692))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6m4Hcg_nq4xj"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxEDYunY_iPw"
      },
      "outputs": [],
      "source": [
        "\n",
        "#create CountVectorizer object.\n",
        "coun_vect=CountVectorizer(stop_words=\"english\", lowercase=True)\n",
        "\n",
        "#generate training BoW vectors.\n",
        "X_train_bow = coun_vect.fit_transform(X_train)\n",
        "\n",
        "#generate test BoW vectors.\n",
        "X_test_bow = coun_vect.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Unique Vocabulary: \",len(coun_vect.vocabulary_))\n",
        "\n",
        "# Print the first 10 features of the count_vectorizer\n",
        "print(coun_vect.get_feature_names()[:10])\n",
        "\n",
        "#convert tf-idf values using numerical matrix format.\n",
        "#X_train_bow_array=X_train_bow.toarray()\n",
        "\n",
        "#print (X_train_bow_array.shape)"
      ],
      "metadata": {
        "id": "tvwr73bhJOdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3WU_zRti_iVc"
      },
      "outputs": [],
      "source": [
        "#generate MultinomialNB object.\n",
        "clf=MultinomialNB()\n",
        "\n",
        "#Fit the Naive Bayes classifier to the train set.\n",
        "text_clf = clf.fit(X_train_bow, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MN4uR1waq4_B"
      },
      "outputs": [],
      "source": [
        "#apply the classifier to the test set and calculate the predicted values.\n",
        "print('Predicting...')\n",
        "predicted = text_clf.predict(X_test_bow)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aA59DyVfAbnP"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "print('accuracy_score',metrics.accuracy_score(y_test,predicted))\n",
        "print('Reporting...')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iVLjh7qAbp0"
      },
      "outputs": [],
      "source": [
        "# Precision/Recall/F1-score measures for each element in the test data.\n",
        "print(metrics.classification_report(y_test, predicted, target_names=labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hN4dwtzGAbsP"
      },
      "outputs": [],
      "source": [
        "# Creating  a confusion matrix,which compares the y_test and y_pred.\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "cm = confusion_matrix(y_test, predicted)\n",
        "cm_df = pd.DataFrame(cm,index = ['health','science','entertainment','business'],\n",
        "                     columns = ['health','science','entertainment','business']  \n",
        "                     )\n",
        "\n",
        "#Plotting the confusion matrix\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm_df, annot=True , fmt=\".0f\")\n",
        "plt.title('Confusion Matrix')\n",
        "plt.ylabel('Actal Values')\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XxUDuW2RIBz"
      },
      "source": [
        "# Part 2: Spam Classification\n",
        "\n",
        "### About Data:\n",
        "- There are 2 columns.\n",
        "\n",
        "- The first column:'ham' which signifies that the message is not spam, and 'spam' which signifies that the message is spam.\n",
        "\n",
        "- The second column: The text content of the message."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyJ_onkzRtif"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "#import a file and put it into a Pandas dataframe.\n",
        "message=pd.read_table('/content/gdrive/My Drive/CIS NLP Data Sets/SMSSpamCollection', sep=\"\\t\")\n",
        "\n",
        "#assign column names.\n",
        "message.columns=['label', 'sms_message']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97Dofni9sFCU"
      },
      "outputs": [],
      "source": [
        "# Output printing out first 5 rows\n",
        "message.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_JLryhrsIHy"
      },
      "outputs": [],
      "source": [
        "#How many columns and rows?\n",
        "print (\"Shape:\", message.shape)\n",
        "\n",
        "#Column names?\n",
        "print (\"Column Names\",message.columns.values)\n",
        "\n",
        "count=message['label'].value_counts()\n",
        "print (count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbnbT1laSa__"
      },
      "source": [
        "### 1: Data Preprocessing ###\n",
        "\n",
        "Convert our labels to binary variables, 0 to represent 'ham'(i.e. not spam) and 1 to represent 'spam' for ease of computation. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvsiRqKI1eDN"
      },
      "outputs": [],
      "source": [
        "message['label'] = message.label.map({'ham':0, 'spam':1})\n",
        "\n",
        "#How many columns and rows?\n",
        "print (\"Shape:\", message.shape)\n",
        "\n",
        "#Column names?\n",
        "print (\"Column Names\",message.columns.values)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUozjORR0pNz"
      },
      "outputs": [],
      "source": [
        "# Output printing out first 5 rows\n",
        "message.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzKVxEBLS_HN"
      },
      "source": [
        "### 2: Bag of words ###\n",
        "\n",
        "Covert a collection of documents to a matrix, with each document being a row and each word(token) being the column, and the corresponding (row,column) values being the frequency of occurrance of each word or token in that document.\n",
        "\n",
        "\n",
        "**Please Note:** \n",
        "\n",
        "* The CountVectorizer method automatically converts all tokenized words to their lower case form so that it does not treat words like 'He' and 'he' differently. But let's set `lowercase` parameter as `False` based on the assumption that many spam messages tend to use all-capital words so we would use these as-is.\n",
        "\n",
        "* It also ignores all punctuation so that words followed by a punctuation mark (e.g.'hello!') are not treated differently than the same word(e.g.'hello').To enable this, use `token_pattern` parameter which has a default regular expression which selects tokens of 2 or more alphanumeric characters.\n",
        "\n",
        "* The third parameter to take note of is the `stop_words` parameter. To enable this, set 'stop_words' as english.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlY3nETOVaW7"
      },
      "source": [
        "### 3: Training and testing sets (before we apply Count Vectorizer) ###\n",
        "\n",
        "\n",
        ">>**Instructions:**\n",
        "Split the dataset into a training and testing set by using the train_test_split method in sklearn.\n",
        "* `X_train` is our training data for the 'message' column.\n",
        "* `y_train` is our training data for the 'label' column\n",
        "* `X_test` is our testing data for the 'message' column.\n",
        "* `y_test` is our testing data for the 'label' column\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ohfliks2sImo"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(message['sms_message'], \n",
        "                                                    message['label'],\n",
        "                                                    random_state=0, \n",
        "                                                    test_size=0.25\n",
        "                                                    )\n",
        "\n",
        "print('Number of rows in the total set: {}'.format(message.shape[0]))\n",
        "print('Number of rows in the training set: {}'.format(X_train.shape[0]))\n",
        "print('Number of rows in the test set: {}'.format(X_test.shape[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MjR4Px-WzlF"
      },
      "outputs": [],
      "source": [
        "print (1393/5571)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLtgMmKQXkkZ"
      },
      "source": [
        "### Continue on #2, Applying Bag of Words processing to our dataset. ###\n",
        "\n",
        "* Fit our training data (`X_train`) into `CountVectorizer()` and return the matrix.\n",
        "* Secondly, we have to transform our testing data (`X_test`) to return the matrix. \n",
        "\n",
        "`X_train` is our training data for the 'sms_message' column in our dataset and we will be using this to train our model. \n",
        "\n",
        "`X_test` is our testing data for the 'sms_message' column and this is the data we will be using(after transformation to a matrix) to make predictions on. We will then compare those predictions with `y_test` in a later step. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WnbPsRSUsIqe"
      },
      "outputs": [],
      "source": [
        "#apply Count Vectorizer.\n",
        "\n",
        "#we are learning a vocabulary dictionary for the training data and then transforming the data into a document-term matrix.\n",
        "#Then, for the testing data, we are only transforming the data into a document-term matrix.\n",
        "\n",
        "# Instantiate the CountVectorizer method\n",
        "tf_vector = TfidfVectorizer(lowercase=False,\n",
        "stop_words='english',\n",
        "use_idf=True)\n",
        "\n",
        "# Fit the training data and then return the matrix\n",
        "training_data = tf_vector.fit_transform(X_train)\n",
        "\n",
        "# Transform testing data and return the matrix. Note we are not fitting the testing data into the CountVectorizer()\n",
        "testing_data = tf_vector.transform(X_test)\n",
        "\n",
        "#https://stackoverflow.com/questions/23838056/what-is-the-difference-between-transform-and-fit-transform-in-sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUSnUW5LsK6L"
      },
      "outputs": [],
      "source": [
        "print (\"Shape of training set\",training_data.shape)\n",
        "\n",
        "print (\"Shape of testing set\",testing_data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sU-m50kY7D9"
      },
      "source": [
        "### 4: Apply NB Algorithm ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pheybn-LYfgW"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "naive_bayes = MultinomialNB()\n",
        "\n",
        "naive_bayes.fit(training_data, y_train)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKIpQZiJZVkc"
      },
      "outputs": [],
      "source": [
        "#predict the labels for testing set.\n",
        "predictions = naive_bayes.predict(testing_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usD3Ihk7ZofJ"
      },
      "source": [
        "### 5: Evaluate the Model. ###\n",
        "\n",
        "Accuracy, precision, recall, F1 score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPl4-yiWZ0nX"
      },
      "source": [
        "\n",
        "- Accuracy  \n",
        "measures how often the classifier makes the correct prediction. It’s the ratio of the number of correct predictions to the total number of predictions.\n",
        "\n",
        "- Precision \n",
        "what proportion of messages we classified as spam, actually were spam.\n",
        "It is a ratio of true positives(words classified as spam, and which are actually spam) to all positives(all words classified as spam, irrespective of whether that was the correct classification).\n",
        "\n",
        "`[True Positives/(True Positives + False Positives)]`\n",
        "\n",
        "- Recall(sensitivity)\n",
        "what proportion of messages that actually were spam were classified by us as spam.<br>\n",
        "It is a ratio of true positives(words classified as spam, and which are actually spam) to all the words that were actually spam.\n",
        "\n",
        "`[True Positives/(True Positives + False Negatives)]`\n",
        "\n",
        "For classification problems that are skewed in their classification distributions like in our case, (e.g. among 100 text messages and only 2 were spam) accuracy by itself is not a very good metric. <br><br>We could classify 90 messages as not spam(including the 2 that were spam but we classify them as not spam, hence they would be false negatives) and 10 as spam(all 10 false positives) and still get a reasonably good accuracy score. For such cases, precision and recall come in very handy. These two metrics can be combined to get the F1 score, which is weighted average of the precision and recall scores. This score can range from 0 to 1, with 1 being the best possible F1 score.\n",
        "\n",
        "For all 4 metrics whose values can range from 0 to 1, having a score as close to 1 as possible is a good indicator of how well our model is doing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qG5eTPBzZVnq"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "print('Accuracy score: ', format(accuracy_score(y_test, predictions)))\n",
        "print('Precision score: ', format(precision_score(y_test, predictions)))\n",
        "print('Recall score: ', format(recall_score(y_test, predictions)))\n",
        "print('F1 score: ', format(f1_score(y_test, predictions)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ng8uEuc612E9"
      },
      "outputs": [],
      "source": [
        "# Precision/Recall/F1-score measures for each element in the test data.\n",
        "print(metrics.classification_report(y_test, predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVJu6p-aZzZJ"
      },
      "outputs": [],
      "source": [
        "# Creating  a confusion matrix,which compares the y_test and y_pred.\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "cm = confusion_matrix(y_test, predictions)\n",
        "cm_df = pd.DataFrame(cm,index = ['ham','spam'],\n",
        "                     columns = ['ham','spam']  \n",
        "                     )\n",
        "\n",
        "#Plotting the confusion matrix\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(cm_df, annot=True , fmt=\".0f\")\n",
        "plt.title('Confusion Matrix')\n",
        "plt.ylabel('Actal Values')\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WpgBaX2H2LnW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}