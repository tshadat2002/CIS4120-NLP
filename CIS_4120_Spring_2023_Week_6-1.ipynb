{"cells":[{"cell_type":"markdown","metadata":{"id":"YFhIktmFmU_P"},"source":["## Week 6 Sentiment Analysis"]},{"cell_type":"markdown","metadata":{"id":"y2A-H8PS6cn3"},"source":["###connect Colab to your Google Drive."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F5ZeYgeZmUYs"},"outputs":[],"source":["#connect Colab to your Google Drive.\n","from google.colab import drive\n","import os\n","drive.mount('/content/gdrive')"]},{"cell_type":"markdown","metadata":{"id":"uc1o917Ezn46"},"source":["### Import IMDB Data Set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cf1Sujibw506"},"outputs":[],"source":["# Original Data Source\n","# https://ai.stanford.edu/~amaas/data/sentiment/\n","# https://www.imdb.com/interfaces/\n","\n","# The same data source in a CSV format from Kaggle.\n","# https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews/version/1"]},{"cell_type":"markdown","metadata":{"id":"jaz0cnGazsay"},"source":["### As always, two different ways to load the files.\n","\n","(1) save the file on your local computer and load it from there on your Jupyter Notebook.\n","\n","(2) Save it in a cloud drive (Google drive) and use your Cloud Python (e.g. Colab) to load the file directly from your drive."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tBBvIOVgw59C"},"outputs":[],"source":["# In my case, I take the (2) approach.\n","\n","import pandas as pd\n","\n","#import csv file and put it into a Pandas dataframe.\n","movie=pd.read_csv('/content/gdrive/My Drive/CIS NLP Data Sets/IMDB Dataset.csv')\n","\n","#assign column names. -> I don't need to do this since we already have col names in the file.\n","#news.columns=[\"col name\"]\n","\n","#movie.head()\n","print (movie.iloc[:10,:])\n","\n","#How many columns and rows?\n","print (\"Shape:\", movie.shape)\n","\n","#Column names?\n","print (\"Column Names\",movie.columns.values)"]},{"cell_type":"markdown","metadata":{"id":"-yc5L1TZ0guf"},"source":["### About Data:\n","IMDB dataset having 50K movie reviews for natural language processing or Text analytics. \n"]},{"cell_type":"markdown","metadata":{"id":"ec-oCIvZ0_yX"},"source":["This is a dataset for binary sentiment classification (positive and negative labels). <br> We provide a set of 25,000 highly polar movie reviews for training and 25,000 for testing. <br> So, predict the number of positive and negative reviews using either classification or deep learning algorithms."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GM3SjjjY1aaH"},"outputs":[],"source":["#What values exist within category?\n","categories = movie['sentiment']\n","\n","labels = list(set(categories))\n","print('possible categories',labels)\n","\n","\n","#Check the frequency of each class label.\n","count=movie['sentiment'].value_counts()\n","print (count)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"jVwObKNd2Nnx"},"source":["We can infer that we will need to encode class labels as numbers.<br><br>\n","e.g. positive -> 1 & negative -> 0"]},{"cell_type":"markdown","metadata":{"id":"69UvAV2EPrFt"},"source":["## How to Build a Sentiment Analysis Algorithm."]},{"cell_type":"markdown","metadata":{"id":"fK-5_nJ_PrP6"},"source":["### 1. Regular ML Approach."]},{"cell_type":"markdown","metadata":{"id":"xbyvmc_Y97jG"},"source":["Requirements\n","\n","- Need pre-labels for each document.\n","- Go through training and testing steps."]},{"cell_type":"markdown","metadata":{"id":"ZWx4S7XU_hZY"},"source":["### 1-1: Data Preprocessing ###\n","\n","Convert our labels to binary variables, 1 to represent 'positive' and 0 to represent 'negative' for ease of computation. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kv6N5dkd_grs"},"outputs":[],"source":["movie['label_num'] = movie.sentiment.map({'negative':0, 'positive':1})\n","\n","#How many columns and rows?\n","print (\"Shape:\", movie.shape)\n","\n","#Column names?\n","print (\"Column Names\",movie.columns.values)\n","\n","#movie.head()\n","print (movie.iloc[:10,:])"]},{"cell_type":"markdown","metadata":{"id":"l7VN5jmzA972"},"source":["### 1-2: Training and testing sets (before we apply Count Vectorizer) ###\n","\n","- Now we should split our data into two sets:\n","1. a training set (75%) used to discover potentially predictive relationships, and\n","2. a test set (25%) used to evaluate whether the discovered relationships hold and to assess the strength and utility of a predictive relationship.\n","\n",">>**Instructions:**\n","Split the dataset into a training and testing set by using the train_test_split method in sklearn.\n","* `X_train` is our training data for the 'review' column.\n","* `y_train` is our training data for the 'label_num' column\n","* `X_test` is our testing data for the 'review' column.\n","* `y_test` is our testing data for the 'label_num' column\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8aWUmAkjBPJb"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(movie['review'], \n","                                                    movie['label_num'],\n","                                                    random_state=0, \n","                                                    test_size=0.25 #assign 25% to a test set.\n","                                                    )\n","\n","print('Number of rows in the total set: {}'.format(movie.shape[0]))\n","print('Number of rows in the training set: {}'.format(X_train.shape[0]))\n","print('Number of rows in the test set: {}'.format(X_test.shape[0]))\n","\n","print (12500/50000)"]},{"cell_type":"markdown","metadata":{"id":"jiXB7-0lBdOS"},"source":["### 1-3: Feature Extration ###\n","\n","Covert a collection of documents to a matrix, with each document being a row and each word(token) being the column, and the corresponding (row,column) values being the frequency of occurrance of each word or token in that document and then apply tf-idf to give different weights to words (tf-idf).\n","\n","**Please Note:** \n","\n","* The CountVectorizer method automatically converts all tokenized words to their lower case form so that it does not treat words like 'He' and 'he' differently. To enable this, set `lowercase` parameter as `True`.\n","\n","* It also ignores all punctuation so that words followed by a punctuation mark (e.g.'hello!') are not treated differently than the same word(e.g.'hello').To enable this, use `token_pattern` parameter which has a default regular expression which selects tokens of 2 or more alphanumeric characters.\n","\n","* The third parameter to take note of is the `stop_words` parameter. To enable this, set 'stop_words' as english.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fvG5oYznPyQh"},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","#generate CountVectorizer object.\n","tfidf_vector = TfidfVectorizer(\n","lowercase=True,                    \n","stop_words='english',\n","ngram_range=(1, 2),             #The lower and upper boundary of the range of n-values for different n-grams to be extracted.\n","max_df=0.3,                     #used for removing terms that appear too frequently\n","min_df=0.05                      #used for removing terms that appear too infrequently.  \n",")\n","\n","\n","#For the entire list of all the parameters:\n","#https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n","\n","#For the details about max_df & min_df: better explanations than the official document:\n","#https://stackoverflow.com/questions/27697766/understanding-min-df-and-max-df-in-scikit-countvectorizer\n","\n","# Be careful with using max_df & min_df:\n","#https://stackoverflow.com/questions/37815899/valueerror-after-pruning-no-terms-remain-try-a-lower-min-df-or-a-higher-max-d\n","\n","# You can also adjust max_features argument along with max_df & min_df\n","\n","# Fit the training data and then return the matrix\n","training_data = tfidf_vector.fit_transform(X_train)\n","\n","# Transform testing data and return the matrix. Note we are not fitting the testing data during the vectorization step!!\n","testing_data = tfidf_vector.transform(X_test)\n","\n","\n","print (\"Shape of training set\",training_data.shape)\n","\n","print (\"Shape of testing set\",testing_data.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-9yJ6LscJTjq"},"outputs":[],"source":["vocab_dict=tfidf_vector.vocabulary_\n","print (\"Unique Vocabulary: \",vocab_dict)\n","print (len(vocab_dict))"]},{"cell_type":"markdown","metadata":{"id":"M2zcNl8hGUlB"},"source":["### 1-4.: Apply ML Model ###"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R6OG7Ay7GT6s"},"outputs":[],"source":["from sklearn.naive_bayes import MultinomialNB\n","\n","#choose a model.\n","naive_bayes = MultinomialNB()\n","\n","#fit your training set to the model.\n","naive_bayes.fit(training_data, y_train)\n","\n","#predict the labels for testing set.\n","predicted = naive_bayes.predict(testing_data)\n"]},{"cell_type":"markdown","metadata":{"id":"k5kpwyAYMKqE"},"source":["### 1-5: Evaluate the Model. ###\n","\n","Accuracy, precision, recall, F1 score"]},{"cell_type":"markdown","metadata":{"id":"-_5dQneXMQyg"},"source":["\n","- Accuracy  \n","measures how often the classifier makes the correct prediction. Itâ€™s the ratio of the number of correct predictions to the total number of predictions.\n","\n","- Precision \n","what proportion of messages we classified as spam, actually were spam.\n","It is a ratio of true positives(words classified as spam, and which are actually spam) to all positives(all words classified as spam, irrespective of whether that was the correct classification).\n","\n","`[True Positives/(True Positives + False Positives)]`\n","\n","- Recall(sensitivity)\n","what proportion of messages that actually were spam were classified by us as spam.<br>\n","It is a ratio of true positives(words classified as spam, and which are actually spam) to all the words that were actually spam.\n","\n","`[True Positives/(True Positives + False Negatives)]`\n","\n","For classification problems that are skewed in their classification distributions like in our case, (e.g. among 100 text messages and only 2 were spam) accuracy by itself is not a very good metric. <br><br>We could classify 90 messages as not spam(including the 2 that were spam but we classify them as not spam, hence they would be false negatives) and 10 as spam(all 10 false positives) and still get a reasonably good accuracy score. For such cases, precision and recall come in very handy. These two metrics can be combined to get the F1 score, which is weighted average of the precision and recall scores. This score can range from 0 to 1, with 1 being the best possible F1 score.\n","\n","For all 4 metrics whose values can range from 0 to 1, having a score as close to 1 as possible is a good indicator of how well our model is doing."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aA59DyVfAbnP"},"outputs":[],"source":["from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","print('Accuracy score: ', format(accuracy_score(y_test, predicted)))\n","print('Precision score: ', format(precision_score(y_test, predicted)))\n","print('Recall score: ', format(recall_score(y_test, predicted)))\n","print('F1 score: ', format(f1_score(y_test, predicted)))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uImG4inqMgle"},"outputs":[],"source":["# Precision/Recall/F1-score measures for each element in the test data.\n","from sklearn.metrics import classification_report\n","\n","print(classification_report(y_test, predicted))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tyjy6f78Mp7U"},"outputs":[],"source":["# Creating  a confusion matrix,which compares the y_test and y_pred.\n","from sklearn.metrics import confusion_matrix\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","cm = confusion_matrix(y_test, predicted)\n","cm_df = pd.DataFrame(cm,index = ['negative','positive'],\n","                     columns = ['negative','positive']  \n","                     )\n","\n","#Plotting the confusion matrix\n","plt.figure(figsize=(6,4))\n","sns.heatmap(cm_df, annot=True , fmt=\".0f\")\n","plt.title('Confusion Matrix')\n","plt.ylabel('Actal Values')\n","plt.xlabel('Predicted Values')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"JPl4-yiWZ0nX"},"source":["### Can you try different ML algorithm to see how the output becomes diffierent from the current output?"]},{"cell_type":"markdown","metadata":{"id":"nfKt24B1NnZ9"},"source":["### 2. Lexicon-based Approach."]},{"cell_type":"markdown","metadata":{"id":"2sqWc3GzNnhe"},"source":["- TextBlob\n","- Vader\n","- Text2emotion for emotion identification\n","\n","** Pre-processing steps before applying one of the approaches."]},{"cell_type":"markdown","metadata":{"id":"6WmFpwFVNnkE"},"source":["2-1. TextBlob"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cb1aa56rRZJe"},"outputs":[],"source":["from textblob import TextBlob"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sJGJdY2SRZqK"},"outputs":[],"source":["#How many columns and rows?\n","print (\"Shape:\", movie.shape)\n","\n","#Column names?\n","print (\"Column Names\",movie.columns.values)\n","\n","#movie.head()\n","print (movie.iloc[100:120,:])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c-PfysM5RZsW"},"outputs":[],"source":["#clean the texts using RE.\n","import regex as re\n","\n","def cleaning(text):\n","# Removes all special characters and numericals leaving the alphabets\n","    text = re.sub('[^A-Za-z]+', ' ', text)\n","    return text\n","\n","# Cleaning the text in the review column\n","movie['clean_review'] = movie['review'].apply(cleaning)\n","\n","#movie.head()\n","print (movie.iloc[100:120,:])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CKEGaJKoRZu8"},"outputs":[],"source":["#apply lower-case function.\n","movie['clean_review']=movie['clean_review'].str.lower()\n","\n","print (movie.iloc[100:120,:])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O_2AR34WRZxQ"},"outputs":[],"source":["#word tokenizer using RegexpTokenizer\n","\n","from nltk.tokenize import regexp_tokenize\n","from nltk import RegexpTokenizer\n","\n","tokenizer_re=RegexpTokenizer(\"[\\w]+\")\n","\n","movie['clean_review']=movie['clean_review'].map(tokenizer_re.tokenize)\n","print (movie.iloc[100:120,:])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HU63ymk5V9Fz"},"outputs":[],"source":["#remove stop-words & one more line of code to remove the words which are shorter than 2 letters.\n","import nltk\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","nltk_stop_words=stopwords.words('english')\n","\n","\n","movie['clean_review']=movie['clean_review'].apply(lambda words: [word for word in words if word not in nltk_stop_words and len(word)>2])\n","print (movie.iloc[100:120,:])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ugSn6D5gcyJR"},"outputs":[],"source":["movie['clean_review_to_string']=movie['clean_review'].apply(lambda x: (' '.join(x)))\n","print (movie.iloc[100:120,:])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vZ26QAoLV9LZ"},"outputs":[],"source":["# Lemmatization.-> This may takes some times.\n","\n","import nltk\n","nltk.download('omw-1.4')\n","#Example of PoS taggings on tokenized sentence.\n","nltk.download('averaged_perceptron_tagger')\n","\n","from nltk.corpus import wordnet\n","from nltk.stem import WordNetLemmatizer\n","nltk.download('wordnet')\n","\n","def get_pos_tags(word):\n","    \"\"\"Map PoS tag to first character lemmatize() accepts\"\"\"\n","    tag = nltk.pos_tag([word])[0][1][0].upper()\n","    tag_dict = {\"J\": wordnet.ADJ, #adjective\n","                \"N\": wordnet.NOUN,#noun\n","                \"V\": wordnet.VERB,#verb\n","                \"R\": wordnet.ADV} #adverb\n","\n","    return tag_dict.get(tag, wordnet.NOUN)\n","\n","\n","def lemmatize_text(text):\n","  text=[WordNetLemmatizer().lemmatize(w, get_pos_tags(w)) for w in text]   \n","  return text\n","\n","movie['clean_review']=movie['clean_review'].apply(lemmatize_text)\n","print (movie.iloc[100:120,:])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1pR9tfoQZPR-"},"outputs":[],"source":["#polarity/subjectivity using TextBlob\n","\n","def polarity(text):\n","  return TextBlob(text).sentiment.polarity\n","\n","def subjectivity(text):\n","  return TextBlob(text).sentiment.subjectivity\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9kh2X56Pa-BB"},"outputs":[],"source":["#pass the data throught the above functions.\n","\n","movie['polarity']=movie['clean_review_to_string'].apply(polarity)\n","movie['subjectivity']=movie['clean_review_to_string'].apply(subjectivity)\n","print (movie.iloc[100:120,:])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZXPiUCVXa-Dc"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","#count the frequency of polarity.\n","num_bins=50\n","plt.figure(figsize=(10,6))\n","n, bins, patches=plt.hist(movie.polarity, num_bins, facecolor='blue')\n","plt.xlabel('polarity')\n","plt.ylabel('count')\n","plt.title('histogram of polarity')\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jge_vK4xdrjA"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","#count the frequency of polarity.\n","num_bins=50\n","plt.figure(figsize=(10,6))\n","n, bins, patches=plt.hist(movie.subjectivity, num_bins, facecolor='green')\n","plt.xlabel('polarity')\n","plt.ylabel('count')\n","plt.title('histogram of subjectivity')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"My9uWSH0emR4"},"outputs":[],"source":["#export some random rows for the manual check-up.\n","random_sample_movie = movie.sample(frac=0.1)\n","\n","random_sample_movie.to_csv('/content/gdrive/My Drive/CIS NLP Data Sets/result_random_sampled.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7fUK_tL4fc8b"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"EdVlz-eLgOvw"},"source":["2-2. Vader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mbyZoKapgQmA"},"outputs":[],"source":["#you might need to pip install first.\n","!pip install vaderSentiment\n","\n","from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OHUspEadgUJf"},"outputs":[],"source":["#initiate the vader sentiment object.\n","analyzer = SentimentIntensityAnalyzer()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jp9rJcEIgvN2"},"outputs":[],"source":["#build a small defined function to generate vader sentiment outputs.\n","def vader_score(text):\n","  return analyzer.polarity_scores(text)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sg49S5hhhGYz"},"outputs":[],"source":["#pass the data throught the above functions.\n","movie['vader_score']=movie['clean_review_to_string'].apply(vader_score)\n","print (movie.iloc[100:120,:])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5lXV6XA_hUrj"},"outputs":[],"source":["#the vader sentiment outputs are stored in a dic format.\n","#you need to pull each key-value pair and need to store each pair into each column.\n","#The below codes will do the work for you. \n","movie['vader_compound']=movie['vader_score'].apply(lambda score_dict: score_dict['compound'])\n","movie['vader_negative']=movie['vader_score'].apply(lambda score_dict: score_dict['neg'])\n","movie['vader_neutral']=movie['vader_score'].apply(lambda score_dict: score_dict['neu'])\n","movie['vader_positive']=movie['vader_score'].apply(lambda score_dict: score_dict['pos'])\n","print (movie.iloc[100:120,:])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZejGqK08ndn8"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","#count the frequency of vader outputs.\n","num_bins=50\n","plt.figure(figsize=(10,6))\n","n, bins, patches=plt.hist(movie.vader_compound    , num_bins, facecolor='green')\n","#plt.xlabel('polarity')\n","plt.ylabel('count')\n","plt.title('histogram of vader_compound')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"6dTykfrei5Yv"},"source":["### Wait: VADER is a module that was specifically created to work with text from a social media contexts."]},{"cell_type":"markdown","metadata":{"id":"AIEc2XBUpCD9"},"source":["If Vader can well understand the sentiments of texts which contains cpital words, punctuations (emphasizing certain words) and so on."]},{"cell_type":"markdown","metadata":{"id":"Mxb7STkVpa6k"},"source":["Why don't we try running Vader module without pre-processing the texts?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LnYgrxm1i42m"},"outputs":[],"source":["#How many columns and rows?\n","print (\"Shape:\", movie.shape)\n","\n","#Column names?\n","print (\"Column Names\",movie.columns.values)\n","\n","#movie.head()\n","print (movie.iloc[100:120,:])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dcDZwB3fi440"},"outputs":[],"source":["#pass the data throught the above functions.\n","movie['vader_score_no_pre_processing']=movie['review'].apply(vader_score)\n","print (movie.iloc[100:120,:])\n","\n","movie['vader_compound_no_pp']=movie['vader_score_no_pre_processing'].apply(lambda score_dict: score_dict['compound'])\n","movie['vader_negative_no_pp']=movie['vader_score_no_pre_processing'].apply(lambda score_dict: score_dict['neg'])\n","movie['vader_neutral_no_pp']=movie['vader_score_no_pre_processing'].apply(lambda score_dict: score_dict['neu'])\n","movie['vader_positive_no_pp']=movie['vader_score_no_pre_processing'].apply(lambda score_dict: score_dict['pos'])\n","print (movie.iloc[100:120,:])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LI59BoyPi47F"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"pze4w3CN1J1E"},"source":["2-3. Text2emotion for emotion identification"]},{"cell_type":"markdown","metadata":{"id":"DzmoHsLa1qD4"},"source":["- Rule-based Algorithm\n","- Detect five different types of emotions such as happy, angry, sad, surprise, fear."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BUdn3J_Ni48-"},"outputs":[],"source":["!pip install text2emotion\n","\n","import text2emotion as emotion\n","\n","!pip uninstall emoji\n","!pip install emoji==1.7"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6DJxjFRM1UT_"},"outputs":[],"source":["#build a small defined function to generate emotion outputs.\n","def emotion_score(text):\n","  return emotion.get_emotion(text)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p25o5RCz2IHO"},"outputs":[],"source":["# Randomly sample 30% of your dataframe\n","movie_random = movie['review'].sample(frac=0.001)\n","movie_random=pd.DataFrame(movie_random)\n","print (\"Shape:\", movie_random.shape)\n","\n","print (\"Column Names\",movie_random.columns.values)\n","\n","#pass the data throught the above functions.\n","#movie['emotion_score']=movie['review_random_sample'].apply(emotion_score)\n","#print (movie.iloc[100:120,:])\n","\n","movie_random['emotion_score']=movie['review'].apply(emotion_score)\n","print (movie_random.iloc[100:120,:])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eXdU3zl52jdK"},"outputs":[],"source":["#break down the dictionary format outputs and insert each component into each column.\n","movie['angry']=movie_random['emotion_score'].apply(lambda score_dict: score_dict['angry'])\n","movie['fear']=movie_random['emotion_score'].apply(lambda score_dict: score_dict['fear'])\n","movie['happy']=movie_random['emotion_score'].apply(lambda score_dict: score_dict['happy'])\n","movie['sad']=movie_random['emotion_score'].apply(lambda score_dict: score_dict['sad'])\n","movie['surprise']=movie_random['emotion_score'].apply(lambda score_dict: score_dict['surprise'])\n","print (movie.iloc[100:120,:])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hHRpvrfg6HMy"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}